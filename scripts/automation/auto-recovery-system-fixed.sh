#!/bin/bash

#############################################
# Auto Recovery System for k8s-ec2-observability
# Step 8: ì‹¤ì „ ìš´ì˜ ìë™í™” & ì‹ ë¢°ì„± ì™„ì„±
# ë§ˆìŠ¤í„°ë…¸ë“œ IP: ip-10-0-1-34 (13.209.82.167)
#############################################

set -euo pipefail

# ì„¤ì • ë³€ìˆ˜
MASTER_NODE="ip-10-0-1-34"
LOG_FILE="/tmp/auto-recovery-$(date +%Y%m%d).log"
CHECK_INTERVAL=30
DRY_RUN=false

# íŒŒë¼ë¯¸í„° ì²˜ë¦¬
if [[ "${1:-}" == "--dry-run" ]]; then
    DRY_RUN=true
    echo "ğŸ§ª DRY RUN MODE - ì‹¤ì œ ë³€ê²½ ì—†ì´ ì ê²€ë§Œ ìˆ˜í–‰"
fi

# ë¡œê¹… í•¨ìˆ˜
log() {
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[${timestamp}] $1" | tee -a "${LOG_FILE}"
}

# ì‹¤í–‰ í•¨ìˆ˜ (DRY RUN ì§€ì›)
execute() {
    local cmd="$1"
    local description="$2"
    
    if [[ "${DRY_RUN}" == "true" ]]; then
        log "ğŸ” [DRY RUN] ${description}: ${cmd}"
        return 0
    else
        log "âš¡ ${description}: ${cmd}"
        eval "${cmd}" 2>&1 | tee -a "${LOG_FILE}" || log "âŒ ì‹¤í–‰ ì‹¤íŒ¨: ${cmd}"
    fi
}

# ë§ˆìŠ¤í„° ë…¸ë“œë¡œ ê°•ì œ ì´ë™ í•¨ìˆ˜
force_to_master() {
    local namespace=$1
    local deployment=$2
    
    log "ğŸ”„ ${deployment} (${namespace})ì„ ë§ˆìŠ¤í„°ë…¸ë“œë¡œ ê°•ì œ ì´ë™ ì¤‘..."
    
    local patch_cmd="kubectl patch deployment ${deployment} -n ${namespace} -p '{
        \"spec\": {
            \"template\": {
                \"spec\": {
                    \"nodeSelector\": {
                        \"kubernetes.io/hostname\": \"${MASTER_NODE}\"
                    },
                    \"tolerations\": [
                        {
                            \"key\": \"node-role.kubernetes.io/control-plane\",
                            \"operator\": \"Exists\",
                            \"effect\": \"NoSchedule\"
                        }
                    ]
                }
            }
        }
    }'"
    
    execute "${patch_cmd}" "ë§ˆìŠ¤í„°ë…¸ë“œ ê°•ì œ ë°°ì¹˜"
}

# 1. OOMKilled Pod ìë™ ë³µêµ¬
check_oomkilled_pods() {
    log "ğŸ” OOMKilled Pod ê²€ì‚¬ ì¤‘..."
    
    local oomkilled_pods
    oomkilled_pods=$(kubectl get pods -A -o json | jq -r '
        .items[] | 
        select(.status.containerStatuses[]?.lastState.terminated.reason == "OOMKilled" and 
               (.metadata.name | test("heartbeat|exporter") | not)) |
        "\(.metadata.namespace)/\(.metadata.name)"
    ' 2>/dev/null | grep -v "^$" || echo "")
    
    if [[ -n "${oomkilled_pods}" ]]; then
        log "ğŸš¨ OOMKilled Pod ë°œê²¬:"
        echo "${oomkilled_pods}" | while IFS= read -r pod_info; do
            if [[ -n "${pod_info}" ]]; then
                namespace=$(echo "${pod_info}" | cut -d'/' -f1)
                pod_name=$(echo "${pod_info}" | cut -d'/' -f2)
                log "   - ${pod_name} (${namespace})"
                
                execute "kubectl delete pod ${pod_name} -n ${namespace}" "OOMKilled Pod ì¬ì‹œì‘"
            fi
        done
    else
        log "âœ… OOMKilled Pod ì—†ìŒ"
    fi
}

# 2. Pending Pod ì²˜ë¦¬ ë° ë¦¬ì†ŒìŠ¤ ìµœì í™”
check_pending_pods() {
    log "ğŸ” Pending Pod ë° ë¦¬ì†ŒìŠ¤ ìƒíƒœ ê²€ì‚¬ ì¤‘..."
    
    local pending_count
    pending_count=$(kubectl get pods -A --field-selector=status.phase=Pending -o json | jq '.items | length' 2>/dev/null || echo "0")
    
    if [[ ${pending_count} -gt 0 ]]; then
        log "âš ï¸  Pending Pod ${pending_count}ê°œ ë°œê²¬"
        
        # CPU ì‚¬ìš©ëŸ‰ í™•ì¸
        local cpu_usage
        cpu_usage=$(kubectl top node ${MASTER_NODE} --no-headers 2>/dev/null | awk '{print $3}' | tr -d '%' || echo "0")
        
        log "ğŸ“Š ë§ˆìŠ¤í„°ë…¸ë“œ CPU ì‚¬ìš©ë¥ : ${cpu_usage}%"
        
        if [[ ${cpu_usage} -gt 90 ]]; then
            log "ğŸš¨ ë†’ì€ CPU ì‚¬ìš©ë¥  ê°ì§€ - ë¦¬ì†ŒìŠ¤ ìµœì í™” ì‹¤í–‰"
            
            # ìš°ì„ ìˆœìœ„ ë‚®ì€ Pod ìŠ¤ì¼€ì¼ ë‹¤ìš´
            execute "kubectl scale deployment reviews-v2 -n bookinfo --replicas=0" "reviews-v2 ìŠ¤ì¼€ì¼ ë‹¤ìš´"
            execute "kubectl scale deployment reviews-v3 -n bookinfo --replicas=0" "reviews-v3 ìŠ¤ì¼€ì¼ ë‹¤ìš´"
        fi
    else
        log "âœ… Pending Pod ì—†ìŒ"
    fi
}

# 3. Linkerd ìƒíƒœ ê²€ì‚¬ ë° ë³µêµ¬
check_linkerd_health() {
    log "ğŸ” Linkerd ìƒíƒœ ê²€ì‚¬ ì¤‘..."
    
    local linkerd_check_result
    linkerd_check_result=$(linkerd check --proxy 2>/dev/null | grep "âˆš" | wc -l || echo "0")
    
    log "ğŸ”— Linkerd ì •ìƒ ì²´í¬: ${linkerd_check_result}ê°œ"
    
    if [[ ${linkerd_check_result} -lt 10 ]]; then
        log "ğŸš¨ Linkerd ìƒíƒœ ì´ìƒ ê°ì§€"
        
        # ì‹¤íŒ¨í•œ Linkerd Pod í™•ì¸ (heartbeat ì œì™¸)
        local failed_linkerd_pods
        failed_linkerd_pods=$(kubectl get pods -n linkerd -o json | jq -r '
            .items[] | 
            select(.status.phase != "Running" and (.metadata.name | test("heartbeat") | not)) |
            .metadata.name
        ' 2>/dev/null | grep -v "^$" || echo "")
        
        if [[ -n "${failed_linkerd_pods}" ]]; then
            log "ğŸ”„ ì‹¤íŒ¨í•œ Linkerd Pod ì¬ì‹œì‘:"
            echo "${failed_linkerd_pods}" | while IFS= read -r pod_name; do
                if [[ -n "${pod_name}" ]]; then
                    log "   - ${pod_name}"
                    execute "kubectl delete pod ${pod_name} -n linkerd" "Linkerd Pod ì¬ì‹œì‘"
                fi
            done
        fi
    else
        log "âœ… Linkerd ìƒíƒœ ì •ìƒ"
    fi
}

# 4. ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ìƒíƒœ ê²€ì‚¬
check_monitoring_health() {
    log "ğŸ” ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ìƒíƒœ ê²€ì‚¬ ì¤‘..."
    
    local prometheus_status
    prometheus_status=$(kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "Unknown")
    
    local grafana_status  
    grafana_status=$(kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "Unknown")
    
    log "ğŸ“Š Prometheus: ${prometheus_status}, Grafana: ${grafana_status}"
    
    if [[ "${prometheus_status}" != "Running" ]] || [[ "${grafana_status}" != "Running" ]]; then
        log "ğŸš¨ ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ì´ìƒ ê°ì§€"
        
        if [[ "${prometheus_status}" != "Running" ]]; then
            execute "kubectl delete pods -n monitoring -l app.kubernetes.io/name=prometheus" "Prometheus ì¬ì‹œì‘"
        fi
        
        if [[ "${grafana_status}" != "Running" ]]; then
            execute "kubectl delete pods -n monitoring -l app.kubernetes.io/name=grafana" "Grafana ì¬ì‹œì‘"
        fi
    else
        log "âœ… ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ì •ìƒ"
    fi
}

# 5. ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ ë³´ê³ 
system_status_report() {
    log "ğŸ“‹ ì‹œìŠ¤í…œ ìƒíƒœ ë³´ê³ ì„œ ìƒì„± ì¤‘..."
    
    local total_pods
    total_pods=$(kubectl get pods -A --no-headers | wc -l)
    
    local running_pods
    running_pods=$(kubectl get pods -A --field-selector=status.phase=Running --no-headers | wc -l)
    
    local error_pods
    error_pods=$(kubectl get pods -A | grep -c "Error\|CrashLoopBackOff\|ImagePullBackOff" || echo "0")
    
    local pending_pods
    pending_pods=$(kubectl get pods -A --field-selector=status.phase=Pending --no-headers | wc -l)
    
    log "ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½:"
    log "   ì „ì²´ Pod: ${total_pods}ê°œ"
    log "   ì‹¤í–‰ ì¤‘: ${running_pods}ê°œ"
    log "   ì˜¤ë¥˜: ${error_pods}ê°œ"
    log "   ëŒ€ê¸° ì¤‘: ${pending_pods}ê°œ"
    log "   ì„±ê³µë¥ : $(( running_pods * 100 / total_pods ))%"
}

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
main() {
    log "ğŸš€ Auto Recovery System ì‹œì‘ (ë§ˆìŠ¤í„°ë…¸ë“œ: ${MASTER_NODE})"
    log "ğŸ“‚ ë¡œê·¸ íŒŒì¼: ${LOG_FILE}"
    
    # ëª¨ë“  ê²€ì‚¬ ì‹¤í–‰
    check_oomkilled_pods
    check_pending_pods  
    check_linkerd_health
    check_monitoring_health
    system_status_report
    
    log "âœ… Auto Recovery System ì™„ë£Œ"
    
    if [[ "${DRY_RUN}" == "true" ]]; then
        log "ğŸ§ª DRY RUN ëª¨ë“œ ì™„ë£Œ - ì‹¤ì œ ë³€ê²½ì‚¬í•­ ì—†ìŒ"
    fi
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@" 